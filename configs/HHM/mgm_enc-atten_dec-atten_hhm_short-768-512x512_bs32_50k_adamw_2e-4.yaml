name: 'mgm_enc-atten_dec-atten_hhm_short-768-512x512_bs32_50k_adamw_2e-4'
output_dir: 'output/HHM'
train:
  batch_size: 30
  num_workers: 10
  max_iter: 50000
  optimizer:
    lr: 2.0e-4
    name: adamw
  scheduler:
    name: cosine
    warmup_iters: 1000
  val_metrics: ['MAD', 'MSE', 'SAD']
  val_best_metric: 'MAD'
  val_iter: 1000
  vis_iter: 200
  log_iter: 50
test:
  num_workers: 4
  metrics: ['MAD', 'MSE', 'SAD', 'Conn', 'Grad']
model:
  arch: 'MGM'
  backbone: 'res_atten_shortcut_encoder_29'
  backbone_args:
    num_mask: 0
  decoder: 'res_shortcut_attention_decoder_22'
  decoder_args:
    final_channel: 32
    atten_dims: [32, 32, 32]
    atten_strides: [4, 2, 2]
    atten_blocks: [2, 2, 2]
    atten_heads: [1, 1, 1]
  mgm:
    warmup_iter: 5000
  loss_dtSSD_w: 0.0
  loss_comp_w: 0.0
dataset:
  train:
    name: 'HHM'
    root_dir: '/mnt/localssd/HHM'
    split: 'train'
  test:
    name: 'HHM'
    root_dir: '/mnt/localssd/HHM'
    split: 'val'