name: 'mgm_swint_atten-dec_hhm_short-768-512x512_bs18_50k_adamw_2e-4'
output_dir: 'output/HHM'
train:
  batch_size: 20
  num_workers: 10
  max_iter: 50000
  optimizer:
    lr: 2.0e-4
    name: adamw
  scheduler:
    name: cosine
    warmup_iters: 1000
  val_metrics: ['MAD', 'MSE', 'SAD']
  val_best_metric: 'MAD'
  val_iter: 1000
  vis_iter: 200
  log_iter: 50
test:
  num_workers: 4
  metrics: ['MAD', 'MSE', 'SAD', 'Conn', 'Grad']
model:
  arch: 'MGM'
  backbone: 'swin_shortcut_t'
  # backbone: 'mit_shortcut_b2_li'
  backbone_args:
    num_mask: 0
  decoder: 'res_shortcut_attention_decoder_22'
  decoder_args:
    final_channel: 32
    atten_dims: [32, 32, 32]
    atten_strides: [4, 2, 2]
    atten_blocks: [2, 2, 2]
    atten_heads: [1, 1, 1]
    num_queries: [0, 0, 0]
    # num_queries: [5, 5, 5]
  mgm:
    warmup_iter: 5000
  loss_dtSSD_w: 0.0
  loss_comp_w: 0.0
dataset:
  train:
    name: 'HHM'
    root_dir: '/mnt/localssd/HHM'
    split: 'train'
  test:
    name: 'HHM'
    root_dir: '/mnt/localssd/HHM'
    split: 'val'