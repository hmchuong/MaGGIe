name: 'mgm_dec-embed-atten_him_short-768-512x512_bs8_50k_adamw_2e-4'
output_dir: 'output/HIM'
train:
  batch_size: 16 #16
  num_workers: 4
  max_iter: 50000
  optimizer:
    lr: 2.0e-4
    name: adamw
  scheduler:
    name: cosine
    warmup_iters: 1000
  val_metrics: ['MAD', 'MSE', 'SAD']
  val_best_metric: 'MAD'
  val_iter: 1000
  vis_iter: 200
  log_iter: 50
  seed: 1105
test:
  num_workers: 4
  metrics: ['MAD', 'MSE', 'SAD']
  log_iter: 1
model:
  arch: 'MGM'
  backbone: 'res_embed_shortcut_encoder_29'
  backbone_args:
    num_mask: 10
  decoder: 'res_shortcut_embed_attention_decoder_22'
  decoder_args:
    max_inst: 10
    final_channel: 32
    atten_dims: [32, 32, 32]
    atten_strides: [4, 2, 2]
    atten_blocks: [2, 2, 2]
    atten_heads: [1, 1, 1]
  mgm:
    warmup_iter: 5000
  loss_dtSSD_w: 0.0
  loss_comp_w: 0.0
dataset:
  train:
    name: 'HIM'
    root_dir: '/mnt/localssd/HHM'
    bg_dir: '/mnt/localssd/bg'
    split: 'train'
  test:
    name: 'HIM'
    root_dir: '/mnt/localssd/HIM2K'
    split: 'comp'